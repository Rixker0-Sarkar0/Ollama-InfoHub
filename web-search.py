import googlesearch
import requests
from bs4 import BeautifulSoup
import subprocess
import os
import textwrap

def google_search_urls(query, num_results=5):
    """
    Performs a Google search and returns a list of URLs.

    Args:
        query (str): The search query.
        num_results (int): Number of search results to retrieve.

    Returns:
        list: A list of URLs, or None if search fails.
    """
    try:
        search_results = googlesearch.search(query, num_results=num_results)
        urls = [result for result in search_results]
        return urls
    except Exception as e:
        print(f"Error during Google Search: {e}")
        return None

def scrape_text_from_url(url):
    """
    Scrapes text content from a given URL, converting HTML to plain text.

    Args:
        url (str): The URL to scrape.

    Returns:
        str: Plain text content from the URL, or None if scraping fails.
    """
    try:
        response = requests.get(url, timeout=10)  # Added timeout to prevent hanging
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Extract text content, removing unwanted tags and scripts.
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()    # rip it out
        text = soup.get_text(separator='\n', strip=True) # Use newline as separator and strip whitespace
        return text
    except requests.exceptions.RequestException as e:
        print(f"Error scraping URL {url}: {e}")
        return None

def chunk_text(text, chunk_size=80000):
    """
    Splits text into chunks of approximately the specified size.

    Args:
        text (str): The text to chunk.
        chunk_size (int): The maximum size of each chunk in characters.

    Returns:
        list: A list of text chunks.
    """
    chunks = []
    start_index = 0
    while start_index < len(text):
        end_index = start_index + chunk_size
        # Avoid breaking words or sentences too abruptly (basic attempt - can be improved)
        if end_index < len(text):
            # Try to find a sentence end (., !, ?) before chunk limit
            sentence_end = text.rfind('.', start_index, end_index)
            if sentence_end == -1:
                sentence_end = text.rfind('!', start_index, end_index)
            if sentence_end == -1:
                sentence_end = text.rfind('?', start_index, end_index)

            if sentence_end > start_index: # Found a sentence end
                end_index = sentence_end + 1 # Include the sentence ending punctuation

        chunks.append(text[start_index:end_index])
        start_index = end_index
    return chunks

def summarize_chunk_with_llama(chunk, model_name="llama3:2b"):
    """
    Summarizes a text chunk using the Llama model via Ollama CLI.

    Args:
        chunk (str): The text chunk to summarize.
        model_name (str): The name of the Ollama model to use.

    Returns:
        str: The summary generated by the Llama model, or None if summarization fails.
    """
    try:
        ollama_command = [
            "ollama", "run", model_name,
            textwrap.dedent(f"""Summarize the following text:

            {chunk}

            Summary:""") # Using textwrap.dedent for cleaner multi-line prompt
        ]

        process = subprocess.Popen(ollama_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        stdout, stderr = process.communicate(timeout=60) # Added timeout to prevent indefinite hanging

        if process.returncode != 0:
            print(f"Ollama command failed with code {process.returncode}")
            if stderr:
                print(f"Ollama stderr: {stderr}")
            return None

        summary = stdout.strip() # Remove leading/trailing whitespace

        # Basic error check within Ollama output (may vary by model) - improve as needed
        if "Error:" in summary or "error:" in summary:
            print(f"Ollama generated an error summary: {summary}")
            return None

        return summary
    except subprocess.TimeoutExpired:
        print("Ollama summarization timed out.")
        process.kill() # Ensure the process is terminated if timeout occurs
        return None
    except FileNotFoundError:
        print("Error: Ollama CLI not found. Make sure it's installed and in your PATH.")
        return None
    except Exception as e:
        print(f"Error during Ollama summarization: {e}")
        return None


def main():
    search_query = input("Enter your search query: ")
    urls = google_search_urls(search_query)

    if not urls:
        print("No URLs found, or Google search failed.")
        return

    print(f"Found URLs: {urls}")

    all_summaries = []
    for url in urls:
        print(f"\nScraping and summarizing content from: {url}")
        scraped_text = scrape_text_from_url(url)

        if scraped_text:
            text_chunks = chunk_text(scraped_text)
            print(f"Text chunked into {len(text_chunks)} parts.")

            chunk_summaries = []
            for i, chunk in enumerate(text_chunks):
                print(f"\nProcessing chunk {i+1}/{len(text_chunks)}...")
                summary = summarize_chunk_with_llama(chunk)
                if summary:
                    chunk_summaries.append(summary)
                    print(f"Chunk {i+1} summary generated.")
                else:
                    print(f"Failed to summarize chunk {i+1}.")

            if chunk_summaries:
                combined_summary = "\n\n".join(chunk_summaries)
                print(f"\nSummaries from URL {url}:\n")
                print(combined_summary)
                all_summaries.append(f"Summary from {url}:\n{combined_summary}")
            else:
                print(f"No summaries generated for URL {url}.")
        else:
            print(f"Failed to scrape text from {url}.")

    if all_summaries:
        final_summary = "\n\n---\n\n".join(all_summaries)
        print("\n\n--- Final Combined Summary ---")
        print(final_summary)
    else:
        print("\nNo summaries generated from any URLs.")

if __name__ == "__main__":
    main()
